### 基本概念

#### 粗排、精排、重排

粗排一般是指用一些比较快速的算法，例如字符串匹配、Embeddings相似度计算等

精排指对粗排后的商品，进行再次排序，使最相关的商品排在前面。考虑用户行为、商品属性等多种特征。常用算法包括XGBoost、GBDT及深度学习模型，如DeepFM

重排进一步优化排序，满足业务需求和用户体验。特点：灵活性高，**考虑业务规则、用户多样性、商品去重**等因素。常考虑的因素如特殊业务规则（库存优先、促销商品优先、新品优先等），多样性优化等

#### XGBooost、GBDT、LightGBM

Gradient Boosting Decision Tree: 梯度提升决策树

GBDT是一种集成学习算法，通过**构建多个弱学习器，通常是决策树**并逐步优化模型性能来完成预测任务

GBDT采用逐步提升的方式，首先构建一棵树，计算它的残差，然后在**残差的基础上训练下一棵树**，依此类推，**每棵树都对之前一轮模型的残差进行拟合**

拟合残差->让模型以输出这个残差值为目标来训练

梯度下降：通过计算损失函数的梯度，指导下一棵树的生长方向，使得整体模型的预测误差逐步减少

Extreme Gradient Boosting：极限梯度提升

XGBoost 是 GBDT 的一种工程优化版本，主要在计算效率和模型效果上进行了改进。它的核心思想依然是利用决策树的集合来进行梯度提升，但在具体实现上做了以下改进：

1. 正则化： XGBoost 在目标函数中加入了正则化项，防止模型过拟合，提高泛化能力。
2. 并行化计算： 通过优化分裂点搜索和树构建过程，XGBoost 支持并行化计算，显著提升了训练速度。
3. 剪枝算法： 在树构建过程中，XGBoost 采用最大深度剪枝，避免生成过复杂的树结构。
4. 缺失值处理： XGBoost可以自动处理缺失值，并且在训练时找到最优分裂方向。

LightGBM: 使用叶子生长（Leaf-wise Growth）的策略。即每次选择损失最大的叶子节点进行扩展。这种方法可以更快地降低损失，但可能会导致树变得不平衡。因此，LightGBM 通常能够生成更深的树，从而在某些场景下提高预测精度。<br>
原生支持类别型特征（Categorical Features），无需额外的编码处理。它会自动处理这些特征并在树的构建过程中进行优化




#### DeepFM

DeepFM是一种结合了因子分解机（Factorization Machines, FM）和深度神经网络（Deep Neural Network, DNN）的推荐系统模型

在推荐系统中，特征通常是**高维且稀疏的，例如用户 ID、商品 ID、类别、标签**等。这些特征之间可能存在复杂的相互关系，比如用户和商品之间的交互

因子分解机（FM）：FM可以有效地捕捉特征之间的**二阶交互关系**，例如用户与商品之间的组合。

深度神经网络（DNN）：DNN擅长自动提取复杂的**高阶特征交互关系**，但通常需要大量数据来训练，并且在捕捉稀疏特征的低阶交互时可能不如FM。

DeepFM 由两个主要组件构成：FM部分和DNN部分。这两个部分共享同一套输入特征嵌入向量，以确保模型可以同时从低阶和高阶特征交互中学习。

1. 输入层（Embedding Layer）
输入特征通常是**稀疏的类别型数据**，比如用户ID、商品ID等。
输入层将这些稀疏的特征转换为密集的低维嵌入向量。
这些嵌入向量**既作为FM部分的输入，也作为DNN部分的输入**。

2. FM 部分（Wide Component）
FM部分主要用于捕捉特征之间的二阶交互。
假设我们有两个特征FM部分会计算它们的交互项，并将所有**二阶交互项累加起来作为输出**。

FM通过计算这些特征嵌入向量之间的点积来衡量二阶交互。举个具体的例子：

假设我们要计算“用户ID: User_A”和“商品 ID: Item_X”之间的二阶交互。具体过程如下：

取出用户ID和商品ID的嵌入向量：

用户ID: [0.2, 0.4, 0.1]<br>
商品ID: [0.5, 0.3, 0.7]

计算点积：

两个向量的点积 = 0.2*0.5 + 0.4*0.3 + 0.1*0.7 = 0.1 + 0.12 + 0.07 = 0.29
这个0.29就是“用户ID: User_A”和“商品ID: Item_X”之间的二阶交互结果。它反映了这两个特征组合在一起对预测的贡献。

**在 DeepFM 模型中，你通过计算所有二阶特征交互来评估各个特征组合对点击率（CTR）的贡献**<br>
->现在有各种用户特征及商品特征，然后计算所有二阶特征，看总体上这些特征对CTR的贡献

3. DNN 部分（Deep Component）
DNN部分主要用于捕捉更高阶的复杂特征交互。
嵌入向量作为DNN的输入，经过一系列全连接层和激活函数，提取出高阶特征交互信息。
DNN部分的最终**输出是一个向量，表示更复杂的特征交互信息**。

4. 输出层
FM部分的输出和 DNN部分的输出会被合并，然后通过一个**输出层（通常是一个sigmoid函数）生成最终的预测值**。
在**广告点击率预估中，输出值代表用户点击广告的概率**。

优势
1. 统一特征学习：DeepFM通过共享嵌入层，实现了FM和DNN部分对特征的统一学习，这既保留了FM擅长的低阶交互能力，又利用了DNN对高阶特征交互的捕捉能力。
2. 高效性：不同于 Wide & Deep 模型需要单独设计手工特征，DeepFM 不需要人工特征工程，且同时捕捉了低阶和高阶特征交互。
3. 应用广泛：DeepFM广泛应用于CTR（点击率）预估、推荐系统等场景，适用于大规模稀疏数据的处理。

假设你在一个电商网站上推荐商品，输入特征包括用户 ID、商品 ID、类别、价格区间等。

FM部分：可能会捕捉到 “用户 ID 和 商品类别” 之间的二阶交互关系。

DNN部分：可能会识别到复杂的特征交互，比如 “用户 ID 和商品价格区间以及购买历史” 的高阶交互关系。

最终，DeepFM将这些信息结合起来，输出一个推荐概率，表明该用户对某商品的兴趣程度。

#### 倒排索引

倒排索引提供了一种高效的方法来找到包含某个词的所有文档。它的基本思路是将词（Term）作为键，将包含这个词的文档列表作为值，形成一种映射关系。


正排索引（Forward Index）：以文档为主索引，列出文档中包含的所有词汇。这适合直接获取某个文档的内容。<br>
例如：文档A包含词汇["apple", "banana", "cherry"]。<br>
倒排索引（Inverted Index）：以词汇为主索引，列出包含这些词汇的所有文档。这样可以快速找到包含特定词汇的所有文档。<br>
例如：词汇 "apple" 对应的文档列表可能是 [文档A, 文档B]。<br>

**将每个词汇作为索引项（键），将包含该词汇的文档 ID 作为索引值（通常是一个列表）**


#### 双塔模型

通过两个独立的神经网络分别对用户和物品（或查询和文档）进行特征编码，然后将它们映射到同一个向量空间中，以便计算相似度或相关性-类似CLIP的逻辑？

双塔模型由两部分组成：

用户塔（User Tower）： 用于编码用户特征的神经网络。<br>
物品塔（Item Tower）： 用于编码物品特征的神经网络。



### Query Processor模块

#### Query类目预测

对用户query进行分类，判断用户的商品类目意图，可在粗排、精排阶段，辅助进行query和item相关性的计算，或者应用于搜索产品辅助功能中的导航、相关搜索等->**Query 类目预测本质上是一个文本分类问题**

1. 基于统计-传统机器学习方法

TF-IDF + 机器学习模型: 将Query转换为TF-IDF特征向量<br>
使用传统分类模型，如朴素贝叶斯（Naive Bayes）、支持向量机（SVM）、逻辑回归（Logistic Regression）等进行分类

词袋模型（Bag of Words）: 将Query表示为词袋模型（Bag of Words），即统计词频，然后输入分类器进行预测

2. 深度学习方法

基于词嵌入（Word Embedding）的模型：<br>
使用预训练的词嵌入模型（如 Word2Vec、GloVe）将 Query 转换为嵌入向量。<br>
然后将这些向量输入到深度神经网络（如 RNN、CNN、LSTM）进行分类

预训练语言模型：<br>
使用 BERT、RoBERTa 等预训练语言模型，直接将 Query 输入模型，获取上下文感知的表示，再通过全连接层输出类目预测结果。
这种方法可以捕捉 Query 中的复杂语义关系，对于**短文本（如搜索 Query）的分类效果较好**。

3. 基于多模态特征的融合

用户行为数据融合：

在仅有文本特征不足以精准预测时，可以引入**用户的历史行为数据（如用户的历史点击、购买记录）**作为辅助特征。<br>
使用推荐系统中的用户嵌入或商品嵌入，将用户和商品特征结合起来进行类目预测

商品属性和图片特征：

如果有用户在搜索时同时查看了某些商品，可以将这些**商品的属性（如价格、品牌、图片等）**作为辅助特征。<br>
商品图片可以通过卷积神经网络（CNN）提取特征，再与文本特征结合进行类目预测。


#### Query改写

分为两个步骤：改写候选召回、改写判别（如GBDT特征融合方法）

改写候选召回常用算法：

1. 基于同义词替换

2. 基于行为统计：共同点击同一个item的query可能为意图一致的query。具体实操可参考经典的推荐i2i召回算法，如协同过滤方式：先构建query到doc的点击矩阵，得到query稀疏表示，进而通过表示距离计算得到源query的相似query

3. 基于内容：源query拓展出相同语义的query，主要有匹配和生成两种方式-**通过构建<query, query>的相似pair对作为训练数据**

匹配：训练深度语义匹配模型（一般为双塔结构，方便向量方式召回候选）<br>
生成：通过seq2seq的模型架构，直接进行query2query的生成

4. 基于session
基于session的方法的query改写可以从两个思路出发，1是用户同一个session内query往往意图相似；2是将用户session内所有行为看作一个doc，query看作doc中的词，类似地，相同上下文的query很可能具有相同的语义。第一个思路的具体实现可简单通过频率统计的方法，第二个思路则可参考word2vec或上文所述的生成模型方法

#### Query term weighting

用户输入的query中不同的term往往有着不同的重要程度，对于搜索匹配的要求也不同。如query “家用的跑步机”，“跑步机”为品类词，权重最高、“家用”为属性词，权重次之、“的”为助词权重最低

1. 静态term重要性方法：依赖特征多为term粒度的离线统计特征（如IDF）

2. 动态term重要性计算：通过特征工程得到可能对term重要性预测相关的特征，然后通过机器学习方法对term权重进行拟合，相应地可分为传统方法和深度学习方法两种


### 召回搜索模块


#### 多路召回架构

##### 文本召回：建立正排和倒排两套索引，倒排索引提供term到item_id列表的映射，正排索引则提供item_id到商品特征和基础信息的映射

基于文本的召回可以提供简单的基线，但由于召回要求term精准命中，常常会出现召回能力不足的问题，因此通常要配合以扩召回策略。常见的扩召回（基于文本）通常从两个维度出发：一是query侧、另一端则是物料侧（商品）->物料侧的扩召回通常是扩充召回字段，如**增加商品详细描述、商品品类、属性等基础文本字段**，或通过相似商品的基础文本信息作为本商品的扩充索引字段

##### 结构化召回：商品结构化信息包括类目、属性等，属性有多个维度如品牌、型号、尺寸、适用性别年龄等，同样可以直接通过结构化id索引召回。

该方法除了应用到基础召回，通常还用在搜索导航功能，如根据层级类目导购、根据属性进行细选等

##### 个性化召回：基于用户行为的召回方法

q2i2i：获取当前query下曝光、点击、成交的item，进一步通过i2i的方法（协同过滤、swing等）扩召回更多候选商品。同时通过类目限制（要求trigger item和cand item的类目一致）或其他离线相关性度量方法保证召回商品和搜索query的相关性，也可以直接通过query和召回商品的相关性度量保证相关性。->**就是看其他输入这个query的人，买了哪些东西，把这些东西也召回**

u2i2i：通过user历史曝光、点击item，召回更多的候选商品，并通过相关性策略进行不相关商品过滤 -> **看这个user的历史搜索和购买记录来召回商品


##### 向量召回

向量召回可以实现基于隐式语义的相关召回

##### 模型

DSSM（Deep Structured Semantic Models）：DSSM使用神经网络分别将表示query和doc表示成向量，向量的距离则可以用来度量二者的匹配程度，如Sentence-BERT

##### 特征

1. 特征增强：Back-translation，随机替换，打乱词序，对抗攻击（梯度sign值/均一化作为扰动）

2. 特征融合：通过**字、词、短语多粒度特征**分别建模，而后进行融合来增加特征的丰富度；也可通过字、词、短语分层表示的方式融合多粒度的特征，如ELMO是典型的考虑上下文特征的字、词、短语层级表示结构


#### 个性化（Personalization）目标
















