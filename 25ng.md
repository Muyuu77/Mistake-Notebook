1. 熟练使用Linux、熟练掌握Shell脚本

2. 熟悉基本NLP库，如nltk，jieba， coreNLP等

##### tokenization, POS, NER...


3. 熟悉图神经网络，有知识图谱建模者优先

##### 了解如何从数据中抽取实体和关系，以及如何使用工具和框架（如Neo4j、Apache Jena）来构建知识图谱。
##### 学习如何编写查询来检索和操作知识图谱中的数据


4. 对主流文本模型(如BERT、GPT3、ChatGPT、T5、PaLM、LLaMA、GLM等)的原理、性能和应用有深入理解

5. 熟悉至少一种常见的大数据工具（如Hadoop/Hive/Spark等）

6. 熟悉Kubernetes及其生态系统，了解容器化技术如Docker

7. 有以下至少一项的背景知识或经验：分布式训练、CUDA算子优化、训练或推理框架、在线推理服务

8. 熟悉大语言模型(LLM)预训练/微调技术，了解相关技术细节和优化策略，如大规模语料收集、模型调优优化、自弱监督学习、强化学习等

9. Rotary Position Embedding

10. KV cache

11. flash-attention

One requirement to use KV cache is for the **positional encoding** for the words that had already been generated, **to not change when new words are generated** (which absolute positional encoding provides)

Relative positional encodings are therefore not suitable for inferencing because **the embeddings for each token change for each new time step**.



## llama3.1

1. 解码器结构+Adapters: Opted for a standard **decoder-only** transformer model architecture with **minor adaptations** rather than a mixture-of-experts model to maximize training stability

2. 迭代SFT+DPO: Adopted an **iterative post-training procedure**, where each round uses **supervised fine-tuning** (SFT) and **direct preference optimization** (DPO). This enabled us to create the highest quality synthetic data for each round and improve each capability’s performance.

3. 用大规模的模型来生成post-trianing数据，优化小模型We also *used the 405B parameter model** to **improve** the post-training quality of our **smaller models**. -> Scaling laws

4. 将模型中的参数量化，减少计算成本和推理成本: To support large-scale production inference for a model at the scale of the 405B, we **quantized our models from 16-bit (BF16) to 8-bit (FP8) numerics**, effectively lowering the compute requirements needed and allowing the model to run within a single server node.

5. In post-training, we produce final chat models by doing **several rounds of alignment on top of the pre-trained model**. Each round involves Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO). We use **synthetic data generation** to produce the **vast majority of our SFT examples**, iterating multiple times to produce higher and higher quality synthetic data across all capabilities. Additionally, we invest in **multiple data processing techniques to filter this synthetic data to the highest quality**. This enables us to scale the amount of fine-tuning data across capabilities.


## 大模型数据预处理

1. n-gram覆盖率去除日志及报错信息->计算一行中重复的n-gram覆盖率，去除覆盖率高的行

2. 用Kullback-Leibler散度来过滤掉含有太多训练集意外特殊token的文档->对比训练集和文档的token分布

3. Model-based quality filtering->用额外的分类器模型来分出高质量文档

4. 对于code和reasoning的数据，使用被Llama2标注的网络文本训练的DistilledRoberta模型，加入了一些包含数学推导和逻辑推理的prompt tuning

**用以往的大模型来过滤、迭代训练数据**

5. Data mix summary: 50% general knowledge + 25% mathematical and reasoning + 17% code tokens + 8% multilingual tokens















