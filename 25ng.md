1. 熟练使用Linux、熟练掌握Shell脚本

2. 熟悉基本NLP库，如nltk，jieba， coreNLP等

##### tokenization, POS, NER...


3. 熟悉图神经网络，有知识图谱建模者优先

##### 了解如何从数据中抽取实体和关系，以及如何使用工具和框架（如Neo4j、Apache Jena）来构建知识图谱。
##### 学习如何编写查询来检索和操作知识图谱中的数据


4. 对主流文本模型(如BERT、GPT3、ChatGPT、T5、PaLM、LLaMA、GLM等)的原理、性能和应用有深入理解

5. 熟悉至少一种常见的大数据工具（如Hadoop/Hive/Spark等）

6. 熟悉Kubernetes及其生态系统，了解容器化技术如Docker

7. 有以下至少一项的背景知识或经验：分布式训练、CUDA算子优化、训练或推理框架、在线推理服务

8. 熟悉大语言模型(LLM)预训练/微调技术，了解相关技术细节和优化策略，如大规模语料收集、模型调优优化、自弱监督学习、强化学习等



## llama3.1

1. 解码器结构+Adapters: Opted for a standard **decoder-only** transformer model architecture with **minor adaptations** rather than a mixture-of-experts model to maximize training stability

2. 迭代SFT+DPO: Adopted an **iterative post-training procedure**, where each round uses **supervised fine-tuning** (SFT) and **direct preference optimization** (DPO). This enabled us to create the highest quality synthetic data for each round and improve each capability’s performance.

3. 用大规模的模型来生成post-trianing数据，优化小模型We also *used the 405B parameter model** to **improve** the post-training quality of our **smaller models**. -> Scaling laws

4. 将模型中的参数量化，减少计算成本和推理成本: To support large-scale production inference for a model at the scale of the 405B, we **quantized our models from 16-bit (BF16) to 8-bit (FP8) numerics**, effectively lowering the compute requirements needed and allowing the model to run within a single server node.

5. In post-training, we produce final chat models by doing **several rounds of alignment on top of the pre-trained model**. Each round involves Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO). We use **synthetic data generation** to produce the **vast majority of our SFT examples**, iterating multiple times to produce higher and higher quality synthetic data across all capabilities. Additionally, we invest in **multiple data processing techniques to filter this synthetic data to the highest quality**. This enables us to scale the amount of fine-tuning data across capabilities.


**Rejection Sampling**:











