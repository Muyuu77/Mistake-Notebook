## Transformer
#### 1. BERT基本结构

Transformer的Encoder，PE + Self-attention + layernorm + (residual + FFN)

#### 2. Self-Attention公式

Attention = Softmax[(Q*K^T)/(dk)^(1/2)] *V


#### 3. BERT预训练任务

MLM: Masked language model 学习上下文信息

NSP: Next sentence pretiction 学习句子间关系

#### 4. BERT之前广泛用哪些方式解决文本分类
#### 5. BERT开头为什么要加一个[CLS]？有什么替代方法？

[CLS]提供了一种下游任务特征提取的渠道。
在预训练时，捕捉整个句子中泛化特征。但是在fine tuning时，根据具体下游任务，捕捉相应的任务特征。

替代方法：
(1) 平均或者池化整个序列token向量来表示整个序列的信息
(2) 选择序列中已有的某个token作为代表性特征

#### 6. BERT有哪些地方用到了Mask？各有什么作用？

预训练MLM任务的时候用到了Mask,用Mask掩盖15%的单词，使模型根据上下文信息来预测被掩盖的单词。
 
#### 7. BERT中Self-Attention计算复杂度是什么？怎么处理长文本？

序列长度n和模型维度d 计算复杂度为O(n^2*d)

分段处理：将长文本切分为较短的片段，然后分别对每个片段应用BERT模型。这种方法简单直接，但可能会丢失跨片段的上下文信息。

使用稀疏注意力机制：一些研究提出了稀疏版的Transformer，其中注意力机制不再关注序列中的每一个位置，而是只关注一部分位置。这样可以显著降低计算复杂度，使模型能够处理更长的序列。

#### 8. 训练Transformer-based模型中重要的超参数？调整超参数的原则？

Maxlen? Multi-head? Hidden state dim? Regularization

#### 9. 为什么Transformer比RNN好？为什么能有更高并行度？

Transformer模型在多个方面优于传统的循环神经网络（RNN），包括长期依赖问题的处理、并行计算能力以及训练大规模数据集的效率。这些优势使得Transformer成为了自然语言处理（NLP）等领域的主流模型之一。下面详细解释Transformer相比于RNN的优势：

##### 1. 长期依赖问题

- **RNN:** RNN处理序列数据时，信息需要逐步传递通过网络的每个时间步。这种结构在理论上可以捕捉长距离依赖，但在实际应用中，由于梯度消失和梯度爆炸问题，RNN难以学习到长距离序列中的依赖关系。
  
- **Transformer:** Transformer通过自注意力机制（Self-Attention Mechanism）直接计算序列中任意两个位置之间的依赖关系，无需像RNN那样逐步传递信息。这使得模型能够更有效地捕捉长期依赖，同时避免了梯度消失和爆炸问题。

##### 2. 并行计算

- **RNN:** 在RNN中，当前时间步的计算依赖于前一个时间步的输出，这种序列依赖性导致训练过程难以并行化。因此，RNN在处理长序列时会遇到显著的计算瓶颈。
  
- **Transformer:** Transformer的自注意力机制允许模型在处理序列数据时，对序列中的所有元素进行同时计算。这意味着在模型训练和推理时，可以对整个序列进行并行处理，显著提高了计算效率。

##### 3. 缩放能力

- **RNN:** RNN的扩展性受到其序列化计算特性的限制，处理大规模数据集或长序列时效率较低。
  
- **Transformer:** Transformer模型设计更易于在现代并行计算设备（如GPU和TPU）上扩展。其自注意力和前馈网络的并行计算特性使得Transformer能够有效地训练更大规模的数据集和模型。

##### 4. 特性提取能力

- **Transformer:** 通过自注意力机制，Transformer能够在计算时自动权衡和选择序列中的重要信息，而不需要显式地通过时间步进行信息传递。这种机制使得Transformer在捕捉序列内复杂模式方面表现更优，尤其是在处理具有复杂结构和语义的自然语言数据时。

总结来说，Transformer模型之所以优于RNN，主要得益于其能够有效解决长期依赖问题、具有更高的并行计算能力和更好的扩展性，以及其强大的特性提取能力。这些优势使得Transformer在自然语言处理和其他序列处理任务中取得了显著的成功。

#### 10. 读bert的那篇原论文了吗？说说bert在那篇论文中的两个主要任务。
#### 11. Encoder和Decoder层为什么都需要FFNN，目的是什么？

增加非线形特征，让序列每一个位置独立地学习和应用不同的转换，增强模型对序列每个元素的理解和表示能力。

#### 12. GPT，BERT，BARD的区别
#### 13. 为什么一开始模型微调都用bert
#### 14. BERT做分类任务的几种方式?微调到Down-Stream任务的几种方式(Lora,p-tuning,prefix-tuning,promopting)
#### 15. BERT和T5的区别


## NLP通用模型相关
#### 1. 模型评估指标有哪些
#### 2. 模型指标计算细节
#### 3. 生成模型采样过程中Temperature超参数的作用和常用取值

Temperature是用来调整模型输出的概率分布情况，如果Temperature=1，则输出原本的概率分布

如果Temperature>1,概率分布会变得更平坦，增加低概率事件被选中的机会，使得模型的输出多样性增加，但是可能导致输出质量降低

如果Temperature<1,概率分布变得更尖锐，减少低概率事件被选中机会，增加生成文本的确定性，倾向于重复模型在训练数据中看到更常见的模式

#### 4. 说几种对比学习的损失函数，以及它们的特点和优缺点



## 预处理
#### 1. Word2Vec是什么？有什么弊端？与BERT相比有什么优势？

## 大模型
#### 1. 对于大模型的了解有多少？
#### 2. GPT主要技术原理阐述。
#### 3. ChatGPT如何实现任务泛化和能力涌现的
#### 4. LoRA的作用和原理

Low-Rank Adaptation

#### 5. LoRA的矩阵怎么初始化？为什么要初始化为全0？

## 其他NLP模型
#### 1. BiLSTM和CRF的原理和区别。

BiLSTM由两个LSTM层组成，捕获序列前向和后向的信息

CRF是统计建模方法，在整个序列上建模条件概率，预测考虑到了相邻标签的依赖性

区别：BiLSTM主要关注于通过捕获长期依赖关系来学习序列中每个元素的表示，而CRF关注于模型整个序列的标签分布，特别是标签之间的依赖关系

结合使用： 在一些高级的NLP任务中，BiLSTM和CRF经常结合使用，形成BiLSTM-CRF模型。这种结合模型首先使用BiLSTM来捕获序列中的上下文信息，然后使用CRF层来优化标签序列的预测，利用了BiLSTM在处理序列数据方面的优势和CRF在序列标注中考虑标签依赖性的能力

#### 2. 对于CLIP的了解

通过融合CV和NLP模型，学习text和image的特征向量，将这些向量映射到同一特征空间下，计算出最相似的两个向量进行匹配。

CLIP（Contrastive Language–Image Pre-training）是一个由OpenAI开发的先进的多模态学习模型，它旨在通过同时理解图像和与之相关的文本描述来改善计算机视觉任务的性能。CLIP模型是在2021年公开介绍的，代表了自然语言处理（NLP）和计算机视觉（CV）领域融合的一种趋势，能够在广泛的视觉任务上，无需任务特定训练数据，直接应用预训练模型进行推断。

##### 主要特点和优势

- **多模态理解**：CLIP通过训练一个视觉模型和一个文本模型来理解图像内容和文本描述之间的关系，使得模型能够捕捉到图像和文本之间的丰富语义信息。

- **零样本学习**：CLIP的一个关键特性是其零样本（zero-shot）能力，即模型能够在未经过特定任务训练的情况下，对新任务进行推断。这是通过比较图像和文本描述的嵌入向量来实现的，使得模型能够理解和处理之前未见过的类别或任务。

- **大规模预训练**：CLIP通过在大量的图像-文本对上进行预训练来学习通用的视觉-语言表示，从而能够泛化到各种视觉任务上。

##### 训练方法

CLIP使用对比学习（contrastive learning）方法进行训练。具体来说，它试图将图像和匹配的文本描述靠近，同时将图像和不匹配的文本描述分开。这样，模型学会了理解不同模态之间的语义对应关系。

##### 应用场景

CLIP在多个视觉任务上展现了出色的性能，包括但不限于：

- **图像分类**：在零样本或少样本设置下对图像进行分类。
- **对象检测**：识别图像中的对象，并了解它们的类别。
- **图像搜索**：根据文本描述来搜索匹配的图像。
- **图像-文本匹配**：确定图像和文本描述是否匹配。

CLIP的开发标志着多模态学习领域的一个重要进展，展示了通过大规模多模态预训练，如何显著提高模型在多种视觉任务上的泛化能力和灵活性。



#### 3. 常见的命名实体识别和信息抽取任务是怎么做的

## 业务
#### 1. 搜索推荐算法主要解决的业务问题是什么？
#### 2. 设计一个语义匹配模型：从数据集构建到模型搭建、训练等。

